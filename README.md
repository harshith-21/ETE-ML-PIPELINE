# ğŸš€ ETE-ML-PIPELINE

**End-to-End ML Pipeline** - A production-ready machine learning infrastructure for continuous training, model versioning, and inference with full model provenance tracking.

## ğŸ¯ Project Overview

ETE-ML-PIPELINE is a complete end-to-end machine learning pipeline that demonstrates production-grade MLOps practices for **Criteo Click-Through Rate (CTR) prediction** using:

- **Apache Airflow** - Orchestration and workflow management
- **MinIO** - S3-compatible object storage for artifacts and logs
- **MLflow** - Experiment tracking and model registry
- **BentoML** - Model serving with version tracking
- **FastAPI** - User-facing frontend with prediction UI
- **PostgreSQL** - Backend storage for Airflow and MLflow
- **Kubernetes** - Container orchestration platform

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Flow                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Frontend   â”‚â”€â”€â–º User inputs features
   â”‚  (FastAPI)   â”‚â—„â”€â”€ Returns predictions + model provenance
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   BentoML    â”‚â”€â”€â–º Loads model from MLflow
   â”‚  (Serving)   â”‚    Tracks: version, run_id, run_name, artifact_uri
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   MLflow     â”‚â”€â”€â–º Model registry + experiment tracking
   â”‚  (Registry)  â”‚    Manages: Production/Staging stages
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Airflow    â”‚â”€â”€â–º Orchestrates training pipeline
   â”‚ (Scheduler)  â”‚    DAGs: criteo_training_pipeline
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    MinIO     â”‚â”€â”€â–º Stores: training data, models, logs
   â”‚  (Storage)   â”‚    Buckets: criteo-data, criteo-logs, mlflow-artifacts
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

### ğŸ” **Model Provenance Tracking**
- Every prediction shows which exact model made it
- Displays: Run Name (MLflow nickname), Run ID (hash), Artifact URI
- Full traceability from MinIO artifact path to prediction result

### ğŸ”„ **Automated Training Pipeline**
- Periodic model retraining with Airflow
- XGBoost model training on Criteo CTR dataset
- Automatic model registration and promotion in MLflow

### ğŸ“¦ **Model Versioning**
- All models tracked in MLflow registry
- Production/Staging stage management
- BentoML automatically loads latest Production model

### ğŸ¨ **User-Friendly Frontend**
- Web UI for making predictions
- Real-time model information display
- Shows all registered model versions and their stages

## ğŸš€ Quick Start

### Prerequisites

- Kubernetes cluster (minikube, kind, or cloud provider)
- kubectl configured with cluster access
- Docker (for building custom images)
- Docker Hub account (for pushing images)

### 1. Deploy Infrastructure

```bash
# Deploy all services at once
./main.sh start all

# Or deploy step-by-step
./main.sh start postgres    # PostgreSQL for Airflow + MLflow
./main.sh start minio        # Object storage
./main.sh start airflow      # Orchestration (includes DAGs in ConfigMaps)
./main.sh start mlflow       # Model registry
./main.sh start bento        # Model serving
./main.sh start frontend     # User interface
```

### 2. Access Services

Set up port forwarding to access the UIs:

```bash
# Airflow UI (admin/admin)
kubectl port-forward -n harshith svc/airflow-webserver 8080:8080

# MinIO Console (minio/minio123)
kubectl port-forward -n harshith svc/minio 9090:9090

# MLflow UI
kubectl port-forward -n harshith svc/mlflow 5000:5000

# Frontend (Make Predictions!)
kubectl port-forward -n harshith svc/frontend 8081:8081
```

Then visit:
- **Airflow**: http://localhost:8080
- **MinIO**: http://localhost:9090
- **MLflow**: http://localhost:5000
- **Frontend**: http://localhost:8081 â­

### 3. Run Training Pipeline

1. Navigate to Airflow UI (http://localhost:8080)
2. Find the `criteo_training_pipeline` DAG
3. Click "Trigger DAG" to start training
4. Watch the pipeline: ingest â†’ process â†’ train â†’ register â†’ promote
5. Model appears in MLflow and BentoML automatically loads it

### 4. Make Predictions

1. Open the Frontend (http://localhost:8081)
2. Use sample inputs from `sample_input_frontend.txt`
3. Click "ğŸš€ Predict CTR"
4. See prediction with full model provenance:
   - Model version
   - Run name (e.g., "glamorous-goose-948")
   - Run ID hash (e.g., "6820c410efef...")
   - Artifact URI in MinIO

## ğŸ“¦ Custom Docker Images

All custom images are multi-platform (linux/amd64, linux/arm64) and hosted on Docker Hub.

### Airflow (`harshith21/ete-ml-pipeline-airflow:latest`)

Custom Airflow 2.10.1 with Python 3.11 and ML dependencies:
- MLflow 2.9.2
- XGBoost 2.0.3
- scikit-learn 1.4.2
- pandas 2.1.3
- boto3 (for S3/MinIO)

### BentoML (`harshith21/ete-ml-pipeline-bento:latest`)

Custom BentoML service that:
- Loads models from MLflow registry
- Tracks model metadata (version, run_id, run_name)
- Exposes /predict, /health, /model_info endpoints
- Automatically uses Production stage models

### Frontend (`harshith21/ete-ml-pipeline-frontend:latest`)

FastAPI application with:
- Beautiful prediction UI
- Model provenance display
- Sample data loader
- Real-time model information

### Building Images

```bash
cd custom_dockerfiles

# Build individual service
cd airflow && ./build.sh
cd bento && ./build.sh
cd frontend && ./build.sh

# Or build all at once
./build-all.sh
```

Images are automatically pushed to Docker Hub during build.

## ğŸ› ï¸ Management Commands

The `main.sh` script provides unified service management:

```bash
./main.sh <action> <service>
```

### Actions
- `start` - Deploy/start a service
- `stop` - Stop a service (scale to 0)
- `restart` - Restart a service
- `cleanup` - Remove a service completely (deployments + services + configmaps)
- `status` - Show service status
- `deploy_all` - Deploy all services
- `cleanup_all` - Remove everything

### Services
- `postgres` - PostgreSQL databases
- `minio` - MinIO object storage
- `airflow` - Airflow (scheduler + webserver)
- `mlflow` - MLflow tracking server
- `bento` - BentoML model serving
- `frontend` - FastAPI frontend
- `all` - All services at once

### Examples

```bash
# Start services
./main.sh start postgres
./main.sh start airflow

# Restart to pick up changes
./main.sh restart airflow
./main.sh restart bento

# Check status
./main.sh status all

# Full cleanup
./main.sh cleanup all
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ main.sh                         # Service management script
â”œâ”€â”€ sample_input_frontend.txt       # Sample inputs for testing
â”œâ”€â”€ adminkubeconfig.yaml            # Kubernetes config (gitignored)
â”‚
â”œâ”€â”€ infra-k8s/                      # Kubernetes manifests
â”‚   â”œâ”€â”€ 0.namespace.yaml            # Namespace: harshith
â”‚   â”œâ”€â”€ 1.postgres.yaml             # PostgreSQL for Airflow + MLflow
â”‚   â”œâ”€â”€ 2.airflow.yaml              # Airflow webserver + scheduler
â”‚   â”œâ”€â”€ 2a.airflowconfigmaps.yaml   # DAGs (criteo_training_pipeline)
â”‚   â”œâ”€â”€ 2b.Minio.yaml               # MinIO S3-compatible storage
â”‚   â”œâ”€â”€ 3.mlflow.yaml               # MLflow tracking + registry
â”‚   â”œâ”€â”€ 4.bento.yaml                # BentoML serving (with ConfigMap)
â”‚   â””â”€â”€ 5.frontend.yaml             # FastAPI frontend
â”‚
â””â”€â”€ custom_dockerfiles/             # Custom Docker images
    â”œâ”€â”€ README.md
    â”œâ”€â”€ build-all.sh
    â”œâ”€â”€ push-all.sh
    â”‚
    â”œâ”€â”€ airflow/                    # Custom Airflow image
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ build.sh
    â”‚   â”œâ”€â”€ build-local.sh
    â”‚   â”œâ”€â”€ push.sh
    â”‚   â”œâ”€â”€ README.md
    â”‚   â””â”€â”€ README-BUILD.md
    â”‚
    â”œâ”€â”€ bento/                      # Custom BentoML image
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ service.py              # BentoML service definition
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ build.sh
    â”‚   â”œâ”€â”€ push.sh
    â”‚   â””â”€â”€ README.md
    â”‚
    â””â”€â”€ frontend/                   # Custom Frontend image
        â”œâ”€â”€ Dockerfile
        â”œâ”€â”€ app.py                  # FastAPI application
        â”œâ”€â”€ requirements.txt
        â”œâ”€â”€ build.sh
        â”œâ”€â”€ templates/
        â”‚   â””â”€â”€ index.html          # Prediction UI
        â””â”€â”€ .dockerignore
```

## ğŸ“ Use Case: Criteo CTR Prediction

The pipeline demonstrates a complete ML workflow for predicting click-through rates:

### 1. **Data Ingestion** (Airflow DAG)
- Downloads Criteo CTR dataset
- Processes and prepares training data
- Stores in MinIO buckets

### 2. **Model Training** (XGBoost)
- Trains on 39 features (13 integer + 26 categorical)
- Logs metrics, parameters, and artifacts to MLflow
- Handles categorical feature encoding automatically

### 3. **Model Registration** (MLflow)
- Registers model with version tracking
- Stores in MinIO (`mlflow-artifacts` bucket)
- Promotes to Production stage

### 4. **Model Serving** (BentoML)
- Loads Production model from MLflow
- Captures metadata: version, run_id, run_name, artifact_uri
- Exposes REST API for predictions

### 5. **User Interface** (FastAPI)
- Accepts 39 feature values
- Returns CTR prediction (%)
- Shows full model provenance

## ğŸ” Model Provenance Example

When you make a prediction, you see:

```
âœ… Prediction: 2.45% CTR

ğŸ” Model Provenance:
â”œâ”€ Model Name: criteo_ctr_model
â”œâ”€ Version: 2
â”œâ”€ Stage: Production
â”œâ”€ Run Name: glamorous-goose-948  (MLflow nickname)
â”œâ”€ Run ID: 6820c410efef45449fbb6ab1044f340c
â””â”€ Artifact URI: s3://mlflow-artifacts/1/6820c410efef45449fbb6ab1044f340c/artifacts/model
```

This hash (`6820c410efef...`) matches exactly what you see in MinIO at:
`http://minio:9090/browser/mlflow-artifacts/1/6820c410efef45449fbb6ab1044f340c/`

## ğŸ”§ Configuration

### Kubernetes Namespace
All services run in the `harshith` namespace.

### Storage Configuration
- **Airflow logs**: MinIO bucket `criteo-logs`
- **Training data**: MinIO bucket `criteo-data`
- **Model artifacts**: MinIO bucket `mlflow-artifacts`

### Default Credentials
- **PostgreSQL (Airflow)**: airflow / airflow
- **PostgreSQL (MLflow)**: mlflow / mlflow
- **MinIO**: minio / minio123
- **Airflow Admin**: admin / admin

### Environment Variables
All services configured via K8s env vars in manifests. Key configurations:
- `MLFLOW_TRACKING_URI`: http://mlflow.harshith.svc.cluster.local:5000
- `MLFLOW_S3_ENDPOINT_URL`: http://minio.harshith.svc.cluster.local:9000
- `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`: MinIO credentials

## ğŸ“š Documentation

- [Custom Docker Images](custom_dockerfiles/README.md)
- [Airflow Image Details](custom_dockerfiles/airflow/README.md)
- [BentoML Service](custom_dockerfiles/bento/README.md)
- [Sample Inputs](sample_input_frontend.txt)

## ğŸ› Troubleshooting

### Pods not starting?
```bash
kubectl get pods -n harshith
kubectl describe pod <pod-name> -n harshith
kubectl logs <pod-name> -n harshith
```

### Image pull errors?
Ensure images are built for correct platform (linux/amd64 or linux/arm64).
Use `./build.sh` which builds multi-platform images automatically.

### Can't access services?
Check port-forward is running and namespace is correct:
```bash
kubectl port-forward -n harshith svc/<service-name> <local-port>:<service-port>
```

### Training pipeline fails?
Check Airflow logs:
```bash
kubectl logs -n harshith -l app=airflow-scheduler --tail=100
```

Check MLflow is running:
```bash
kubectl get pods -n harshith -l app=mlflow
```

---
---

In a shocking plot twist that surprised absolutely no one (least of all me), a kubeconfig accidentally made its way into the repository.
Donâ€™t worry â€” the token has been obliterated, yeeted into the void, ritually deleted, and no longer grants access to anything more powerful than a 404 page.

Mistakes were made.
But hey â€” I fix my mistakes before they become security incidents.
Call it personal growth, call it self-preservation, call it â€œplease donâ€™t revoke my cluster access.â€

The important part:
The leaked kubeconfig is now about as effective as shouting â€œkubectl applyâ€ at a brick wall.

---
---

## ğŸ¤ Contributing

This is a learning/demonstration project showcasing MLOps best practices. Feel free to:
- Fork and adapt for your needs
- Add new features (e.g., A/B testing, canary deployments)
- Improve the pipeline (e.g., add data validation, model monitoring)

## ğŸ“ License

MIT License

## ğŸ”— Links

- **Docker Hub Organization**: https://hub.docker.com/u/harshith21
- **Airflow Image**: https://hub.docker.com/r/harshith21/ete-ml-pipeline-airflow
- **BentoML Image**: https://hub.docker.com/r/harshith21/ete-ml-pipeline-bento
- **Frontend Image**: https://hub.docker.com/r/harshith21/ete-ml-pipeline-frontend

---

**ETE-ML-PIPELINE** - End-to-End Machine Learning Pipeline with Full Model Provenance ğŸš€

Built with â¤ï¸ for MLOps learning and demonstration.
