apiVersion: v1
kind: ConfigMap
metadata:
  name: bento-service
  namespace: harshith
data:
  service.py: |
    """
    BentoML Service for Criteo CTR Model
    Loads the production model from MLflow and serves predictions
    """
    import bentoml
    import numpy as np
    import pandas as pd
    import mlflow
    import os
    from bentoml.io import JSON, NumpyNdarray

    # MLflow configuration
    MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow.harshith.svc.cluster.local:5000")
    MODEL_NAME = os.getenv("MODEL_NAME", "criteo_ctr_model")
    MODEL_STAGE = os.getenv("MODEL_STAGE", "Production")

    # S3/MinIO configuration for MLflow artifacts
    os.environ["AWS_ACCESS_KEY_ID"] = os.getenv("AWS_ACCESS_KEY_ID", "minio")
    os.environ["AWS_SECRET_ACCESS_KEY"] = os.getenv("AWS_SECRET_ACCESS_KEY", "minio123")
    os.environ["MLFLOW_S3_ENDPOINT_URL"] = os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://minio.harshith.svc.cluster.local:9000")

    print(f"Initializing BentoML service...")
    print(f"MLflow Tracking URI: {MLFLOW_TRACKING_URI}")
    print(f"Model Name: {MODEL_NAME}")
    print(f"Model Stage: {MODEL_STAGE}")

    # Set MLflow tracking URI
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    # Load the production model from MLflow
    try:
        model_uri = f"models:/{MODEL_NAME}/{MODEL_STAGE}"
        print(f"Loading model from: {model_uri}")
        model = mlflow.xgboost.load_model(model_uri)
        print(f"✓ Model loaded successfully!")
    except Exception as e:
        print(f"✗ Failed to load model: {e}")
        # Fallback: try to load the latest version
        try:
            client = mlflow.tracking.MlflowClient()
            versions = client.search_model_versions(f"name='{MODEL_NAME}'")
            if versions:
                latest_version = versions[0].version
                model_uri = f"models:/{MODEL_NAME}/{latest_version}"
                print(f"Trying latest version: {model_uri}")
                model = mlflow.xgboost.load_model(model_uri)
                print(f"✓ Model loaded successfully (version {latest_version})!")
            else:
                raise Exception(f"No versions found for model {MODEL_NAME}")
        except Exception as e2:
            print(f"✗ Failed to load model (fallback): {e2}")
            model = None

    # Create BentoML service
    svc = bentoml.Service("criteo_ctr_service")

    @svc.api(input=JSON(), output=JSON())
    def predict(input_data):
        """
        Predict CTR for Criteo ad data
        
        Input format:
        {
            "features": [[feat1, feat2, ..., feat39], ...],  # 39 features per sample
            "feature_names": ["I1", "I2", ..., "C26"]  # Optional
        }
        
        Output format:
        {
            "predictions": [0.123, 0.456, ...],
            "model_version": "Production",
            "model_name": "criteo_ctr_model"
        }
        """
        if model is None:
            return {
                "error": "Model not loaded",
                "model_name": MODEL_NAME,
                "model_stage": MODEL_STAGE
            }
        
        try:
            # Extract features
            features = input_data.get("features", [])
            if not features:
                return {"error": "No features provided"}
            
            # Convert to numpy array or pandas DataFrame
            import xgboost as xgb
            
            # Create DMatrix for prediction
            dmatrix = xgb.DMatrix(np.array(features))
            
            # Make prediction
            predictions = model.predict(dmatrix)
            
            return {
                "predictions": predictions.tolist(),
                "model_name": MODEL_NAME,
                "model_stage": MODEL_STAGE,
                "num_samples": len(predictions)
            }
        
        except Exception as e:
            return {
                "error": str(e),
                "model_name": MODEL_NAME,
                "model_stage": MODEL_STAGE
            }

    @svc.api(input=JSON(), output=JSON())
    def health(data=None):
        """Health check endpoint"""
        return {
            "status": "healthy" if model is not None else "model_not_loaded",
            "model_name": MODEL_NAME,
            "model_stage": MODEL_STAGE,
            "mlflow_uri": MLFLOW_TRACKING_URI
        }

    @svc.api(input=JSON(), output=JSON())
    def model_info(data=None):
        """Get model information"""
        if model is None:
            return {
                "error": "Model not loaded",
                "model_name": MODEL_NAME,
                "model_stage": MODEL_STAGE
            }
        
        try:
            client = mlflow.tracking.MlflowClient()
            versions = client.search_model_versions(f"name='{MODEL_NAME}'")
            
            model_info = {
                "model_name": MODEL_NAME,
                "model_stage": MODEL_STAGE,
                "mlflow_uri": MLFLOW_TRACKING_URI,
                "versions": []
            }
            
            for version in versions:
                model_info["versions"].append({
                    "version": version.version,
                    "stage": version.current_stage,
                    "run_id": version.run_id,
                    "creation_timestamp": str(version.creation_timestamp)
                })
            
            return model_info
        
        except Exception as e:
            return {
                "error": str(e),
                "model_name": MODEL_NAME,
                "model_stage": MODEL_STAGE
            }
---
apiVersion: v1
kind: Service
metadata:
  name: bento-svc
  namespace: harshith
spec:
  selector:
    app: bento-svc
  ports:
    - port: 3000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bento-svc
  namespace: harshith
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bento-svc
  template:
    metadata:
      labels:
        app: bento-svc
    spec:
      containers:
        - name: bento
          image: harshith21/ete-ml-pipeline-bento:latest
          imagePullPolicy: Always
          workingDir: /app
          command: ["python", "-m", "bentoml", "serve", "service:svc", "--host", "0.0.0.0", "--port", "3000", "--production"]
          ports:
            - containerPort: 3000
          env:
            # MLflow configuration
            - name: MLFLOW_TRACKING_URI
              value: http://mlflow.harshith.svc.cluster.local:5000
            - name: MODEL_NAME
              value: "criteo_ctr_model"
            - name: MODEL_STAGE
              value: "Production"
            
            # MinIO S3 configuration (for MLflow artifacts)
            - name: AWS_ACCESS_KEY_ID
              value: "minio"
            - name: AWS_SECRET_ACCESS_KEY
              value: "minio123"
            - name: MLFLOW_S3_ENDPOINT_URL
              value: "http://minio.harshith.svc.cluster.local:9000"
            - name: AWS_REGION
              value: "us-east-1"
          
          volumeMounts:
            - name: bento-service
              mountPath: /app
              readOnly: true
      
      volumes:
        - name: bento-service
          configMap:
            name: bento-service